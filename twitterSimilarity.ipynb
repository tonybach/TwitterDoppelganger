{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Your Doppelganger: Who is Your Most Similar Twitter Friend?\n",
    "### COMP 440: Collective Intelligence\n",
    "### Instructor: Shilad Sen\n",
    "### Eva Yifan Gong, Tony Bach, Gozong Lor\n",
    "\n",
    "##### Introduction\n",
    "\n",
    "This is the our project's programming part. **Our project's objective is to design a program that would find your most similar Twitter friend/your Doppelganger.** Our program calculates the similarity score between you and all friends you followed by looking at three difference aspects: **profile similarity, network similarity and content similarity**. Our program gives you the freedom to assign different weights x, y, z to those three similarity metrics. \n",
    "\n",
    "Our program is divided into two sections. The first section is our main program to calculate and return a given userâ€™s doppelganger. The second section is a quantitative analysis of our program, which also includes the codes we write to generate the graphs presented in our poster.\n",
    "\n",
    "The first section can be divided into four different parts. The first part is to import necessary packages and modules for our use. We also specify our credential in the first part as required by Twitter API. The second part is to collect data on a given user's friends list, profile information, mentions, retweets, all tweets and hashtags and store them in dictionaries. The third part is the calculation part, in which we calculate the cosine similarity and tf-idf scores for different similarity metrics. The last and fourth part is the one to combine all metrics together and return the final result: a given user's doppelganger.\n",
    "\n",
    "##### Overview\n",
    "**_1. Main Program_**\n",
    "\n",
    "*1.1 Set-up*\n",
    "\n",
    "*1.2 Data Collection*\n",
    "\n",
    "*1.3 Calculation*\n",
    "\n",
    "*1.4 Final Return*\n",
    "\n",
    "**_2. Quantitative Analysis_**\n",
    "\n",
    "##### Dependencies\n",
    "Make sure you have the stop words python module installed. For directions on how to install: https://pypi.python.org/pypi/stop-words\n",
    "\n",
    "##### How to Compile the Code\n",
    "This code was written with the intention of seeing it turned into an application with an interactive GUI. In short, users should be able to type in a Twitter username and the app would return the list of the top doppelgangers. We don't have a GUI, but we wanted to mimic this process as closely as possible.\n",
    "\n",
    "To compile the code, make sure all the code in each cell has been run. Then simply create a new code cell at the bottom of this iPython notebook. Copy the code in this blockquote to your cell, change the username 'Macalester' to your desired username, adjust the weights of each metric, then click run.\n",
    "\n",
    "(The first weight corresponds to content. The second weight to profile, and the third weight to network similarity.)\n",
    "\n",
    ">findDoppelganger('Macalester', 2.0, 4.0, 2.0)\n",
    "\n",
    "#### Future Work\n",
    "At the moment, the network analysis is only partial, as it only takes into consideration mentions and not retweets.\n",
    "However, the retweet data gathering code has also been included with comments and description on the intended use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# COMP 440: Collective Intelligence\n",
    "# Instructor: Shilad Sen\n",
    "# Finding Your Doppelganger: Who is Your Most Similar Twitter Friend? \n",
    "# Tony Bach, Eva Yifan Gong, Gozong Lor\n",
    "# Last update: 12/15/2015\n",
    "#######################################################################\n",
    "\n",
    "#######################################################################\n",
    "# 1.1 SET UP\n",
    "# Getting credentials and libraries initialized.\n",
    "#######################################################################\n",
    "\n",
    "import pprint\n",
    "import twitter\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "\n",
    "CONSUMER_KEY = 'HtEfcIvBG9xp8i6kEHvhhHgRG'\n",
    "CONSUMER_SECRET = '8B4Avz38G0CVjxCKFNjLLcICiSvlBd02VxobndJuAsgSBrdCGo'\n",
    "OAUTH_TOKEN = '294030079-BTZ0LzDJedShBJTbP9OOjL8JJRjzqPocyNPQdVzL'\n",
    "OAUTH_TOKEN_SECRET = 'MF7LYJwPe5EpBUzxxbCRrqpQ2H3iPg7pQhR9Ra95lTM7a'\n",
    "\n",
    "# CONSUMER_KEY = 'gdf9ARHNtdaCNZOqUNWTDBC3l'\n",
    "# CONSUMER_SECRET = 'tgXH2BzTApr4SqVzmcgnAE0WlMo8Oc7IofY95aHDmIVpfO38PL'\n",
    "# OAUTH_TOKEN = '3392065319-L9wNY6enpZNJCYQE842qTD8wJMtKDJYlEDyLhPq'\n",
    "# OAUTH_TOKEN_SECRET = '6utP5pBkCz1SWzUbZgbH6pjVdCivdOXlO7KoN3wmdHQhc'\n",
    "\n",
    "auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    " CONSUMER_KEY, CONSUMER_SECRET)\n",
    "\n",
    "t = twitter.Twitter(auth=auth)\n",
    "pp = pprint.PrettyPrinter(indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# 1.2 DATA COLLECTION\n",
    "# All the methods we used to collect our data from Twitter. These methods\n",
    "# do not have any calculations occuring within them.\n",
    "##########################################################################\n",
    "\n",
    "############################################\n",
    "# GET TWEET, DESCRIPTION, LOCATION DATA\n",
    "############################################\n",
    "\n",
    "def get_info_of_self_and_friends(my_user_name):\n",
    "    tweet_dict = defaultdict(list)\n",
    "    description_dict = defaultdict(str)\n",
    "    location_dict = defaultdict(str)\n",
    "    \n",
    "    # getting user's tweets, description and location\n",
    "    tweet_dict[my_user_name] = t.statuses.user_timeline(screen_name=my_user_name, count = 200)\n",
    "    user_info = t.users.show(screen_name=my_user_name)\n",
    "    description_dict[my_user_name] = user_info['description']\n",
    "    location_dict[my_user_name] = user_info['location']\n",
    "    \n",
    "    # number of calls we have left to get a user's tweets. Need to keep track of\n",
    "    # this so that we don't run into the rate limit error\n",
    "    numCallsLeft = t.application.rate_limit_status(resources=\"statuses\")['resources']['statuses']['/statuses/user_timeline']['remaining']\n",
    "    \n",
    "    # friend list is in reverse chronological order, so we want to get to the bottom of the list\n",
    "    friends = t.friends.list(screen_name=my_user_name, count = 200)\n",
    "    while (friends['next_cursor'] != 0):\n",
    "        friends = t.friends.list(screen_name=my_user_name, cursor = friends['next_cursor'], count = 200)\n",
    "    \n",
    "    # go up the list from the bottom, to get the \"oldest\" friends first\n",
    "    # need to leave two calls remaining for another method\n",
    "    while (numCallsLeft >= 2):\n",
    "        i = len(friends['users']) - 1\n",
    "        while (i >= 0):\n",
    "            friend = friends['users'][i]\n",
    "            friend_name = friend['screen_name']\n",
    "            if (friend_name not in tweet_dict):\n",
    "                try:\n",
    "                    friend_tweets = t.statuses.user_timeline(screen_name=friend_name, count = 200)\n",
    "                    friend_info = t.users.show(screen_name=friend_name)\n",
    "                    friend_description = friend_info['description']\n",
    "                    friend_location = friend_info['location']\n",
    "                    if (friend_tweets > 20):\n",
    "                        tweet_dict[friend_name] = friend_tweets\n",
    "                        description_dict[friend_name] = friend_description\n",
    "                        location_dict[friend_name] = friend_location\n",
    "                except twitter.api.TwitterHTTPError:\n",
    "                    pass\n",
    "                numCallsLeft -= 1\n",
    "            i -= 1           \n",
    "        # this is reached when we still have calls left, but we have already iterated through\n",
    "        # all the friends of this user\n",
    "        if (friends['previous_cursor'] == 0):\n",
    "            break\n",
    "        friends = t.friends.list(screen_name=my_user_name, cursor = friends['previous_cursor'], count = 200)\n",
    "        \n",
    "    return tweet_dict, description_dict, location_dict\n",
    "        \n",
    "# bad practice, but we don't want to call method more than once because of the rate limit\n",
    "# for testing purpose only, should not do this when submitting final code\n",
    "tweet_dict, description_dict, location_dict = get_info_of_self_and_friends('Macalester')\n",
    "\n",
    "############################################\n",
    "# GET MENTION DATA\n",
    "############################################\n",
    "\n",
    "# ======================================================\n",
    "# Function find_who_you_mentioned_in_your_tweets(userA)\n",
    "# ======================================================\n",
    "# find_who_you_mentioned_in_your_tweets(userA)\n",
    "# Returns a dictionary of users that userA mentioned and number of times they were mentioned\n",
    "# Reads in a username for userA (i.e. \"gozonglor\", string)\n",
    "# i.e. {@bob: 1, @ann: 4, @cary: 2, etc} --> userA mentioned bob 1 time, ann 4 itmes, and cary 2 times.\n",
    "def find_who_you_mentioned_in_your_tweets(userA):\n",
    "    # SET UP\n",
    "    # Notice below: 'count' is hard-coded to be 200. 200 is the Twitter API limit. More tweets, more mentions.\n",
    "    yourTweets = t.statuses.user_timeline(screen_name=userA, count=200, include_rts=\"false\") \n",
    "    yourMentions = {} #Initialize a dictionary to store all your mentioned users\n",
    "    \n",
    "    # BUILD DICTIONARY OF MENTIONS\n",
    "    for status in yourTweets: # For each of your tweets\n",
    "        if \"@\" in status['text']: # If there's an @ symbol (a mention)\n",
    "            cleanTweet = status['text'].strip() # Clean the entire tweet of any unnecessary white space\n",
    "            tokens = cleanTweet.split(\" \") # Split the tweet into words\n",
    "            for word in tokens: # For each word\n",
    "                if len(word) > 1: # If it's longer than 1 character\n",
    "                    wordS = word.split() # Split the word into individual characters (i.e. from \"@bob\" to [\"@\",\"b\",\"o\",\"b\"])\n",
    "                    if word[0][:1] == \"@\": # If the first character is an @\n",
    "                        if word == \"@@\": # Error handling for typo in a mention?\n",
    "                            break\n",
    "                        if word in yourMentions:\n",
    "                            yourMentions[word] += 1\n",
    "                        else:\n",
    "                            yourMentions[word] = 1\n",
    "                            \n",
    "    # IF yourMentions REMAINS EMPTY...\n",
    "    if len(yourMentions) == 0: \n",
    "        print(\"Sorry, \"+userA+\" has not mentioned anyone yet.\")\n",
    "    return yourMentions\n",
    "\n",
    "# ============================================\n",
    "# Function find_who_mentioned_you(numTweets)\n",
    "# ============================================\n",
    "# Returns a dictionary of users and number of times they mentioned the authenticating user.\n",
    "# numTweets are the number of tweets to pull that mention the AUTHENTICATING user\n",
    "# Twitter API restricts numTweets to be >= 200. \n",
    "def find_who_mentioned_you(numTweets):\n",
    "    # SET UP\n",
    "    tweets = t.statuses.mentions_timeline(count=numTweets)\n",
    "    mentionedBy = {}\n",
    "    \n",
    "    # FIND USERS WHO MENTIONED AUTHENTICATING USER\n",
    "    if len(tweets) == 0:\n",
    "        print(\"Sorry, no mentions of the authenticated user.\")\n",
    "    else:\n",
    "        for tweetObject in tweets: \n",
    "            tweeter = tweetObject['user']['screen_name'] # Tweeter is the user who mentioned your authenticating user\n",
    "            if tweeter in mentionedBy:\n",
    "                mentionedBy[tweeter] += 1\n",
    "            else:\n",
    "                mentionedBy[tweeter] = 1\n",
    "    return mentionedBy\n",
    "\n",
    "# zero_list_maker(n)\n",
    "# Helper function, used to initialize similarity vectors that are filled later\n",
    "def zero_list_maker(n):\n",
    "    listofzeros = [0] * n\n",
    "    return listofzeros\n",
    "\n",
    "# is_user_active(username)\n",
    "# Helper function, checks if a given username is active (return True) or not active (return False)\n",
    "def is_user_active(username):\n",
    "    try:\n",
    "        results = t.users.lookup(screen_name=username) #API query#\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# is_user_protected(username)\n",
    "# Helper function, checks if a given username is protected (return True) or not protected (return False)\n",
    "def is_user_protected(username):\n",
    "    try:\n",
    "        whoTheyMentioned = findWhoYouMentionedInYourTweets(username) # Find who they mentioned in their timeline\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# ==================================================================\n",
    "# Function build_vectors_for_who_mentioned_you(whoYouMentionedDict)\n",
    "# ==================================================================\n",
    "# Returns a dictionary: {john: [[1,2,3][0,1,4]], anna: [[1,2,3][9,3,2]], bill: [[1,2,3][0,0,6]]}\n",
    "# Key: Username of mentioned friend\n",
    "# Value: A list of vectors. Index 0 -> Authenticating user's similarity vector. Index 1 -> Key user's similarity vector.\n",
    "def build_vectors_for_who_mentioned_you(whoYouMentionedDict):\n",
    "    # TO DO: Find a way to handle all the nan (i.e. eva's vector [0,0], and my vector [1,2])\n",
    "    \n",
    "    # SET UP\n",
    "    vectorDict = {} # The dictionary returned at the end of the function (described above)\n",
    "    yourVector = [] # Initializing the authenticating user's similarity vector\n",
    "    yourVector = zero_list_maker(len(whoYouMentionedDict)) # Initialize vector to 0\n",
    "    whoYouMentionedDict = OrderedDict(sorted(whoYouMentionedDict.items(), key=lambda t: t[1])) # We want the dictionary to keep its order\n",
    "    \n",
    "    # BUILD YOUR SIMILARITY VECTOR\n",
    "    for f in whoYouMentionedDict:\n",
    "        friendIndex = whoYouMentionedDict.keys().index(f) # Grab your friend's index in the ordered dictionary\n",
    "        yourVector[friendIndex] = whoYouMentionedDict[f] # Use the index to insert them into your vector\n",
    "        \n",
    "    # BUILD EACH OF YOUR FRIEND'S SIMILARITY VECTOR    \n",
    "    for friend in whoYouMentionedDict: # For each friend you mentioned\n",
    "        # SET UP\n",
    "        friendIndex = whoYouMentionedDict.keys().index(friend) # Grab their index in the ordered dictionary\n",
    "        friendVector = [] # Initialize a similarity vector for your friend that will be stored in the vector dictionary\n",
    "        friendVector = zero_list_maker(len(whoYouMentionedDict))\n",
    "        friend = friend.split(\"@\") # Clean up your friend's username\n",
    "        friend = friend[1]\n",
    "\n",
    "        # 1. MAKE SURE USER IS ACTIVE AND NOT PROTECTED\n",
    "        # 2. GET EACH FRIEND'S MENTIONS THAT INTERSECT WITH THE AUTHENTICATING USER'S MENTIONS\n",
    "        # 3. BUILD THE SIMILARITY VECTOR AND INSERT INTO VECTOR DICTIONARY\n",
    "        if is_user_active(friend) is False: # If this friend is no longer on twitter, keep their vector to all 0s\n",
    "            vectorDict[friend] = [yourVector, friendVector] # Entry in vectorDict (returned dictionary) becomes friend:[[authenticating user's vector],[[vector of 0s]]]\n",
    "        else: # If the user is still active\n",
    "            if is_user_protected(friend) is False: # Check if they are also protected\n",
    "                print(\"This user \"+friend+\" is protected and we can't access their user timeline for tweets.\\n\") # Print error msg.\n",
    "            else: # If the user is not protected\n",
    "                whoTheyMentioned = find_who_you_mentioned_in_your_tweets(friend) # Find who your friend mentioned in their timeline\n",
    "                for friend2 in whoTheyMentioned: # And for each friend of theirs (friend2) that they mentioned\n",
    "                    if friend2 == friend: # If they mention themselves\n",
    "                        score = whoTheyMentioned[friend]\n",
    "                        friendVector[friendIndex] = score  # Store their score in their vector using the friendIndex pulled from the ordered dictionary whoYouMentionedDict \n",
    "                    if friend2 != friend: # If they dont mention themselves/if friend2 is someone else\n",
    "                        if friend2 in whoYouMentionedDict: # And authenticating user mentioned friend2 as well\n",
    "                            friend2Index = whoYouMentionedDict.keys().index(friend2) # Grab the index of the friend2 in your dictionary/vector\n",
    "                            friendVector[friend2Index] = whoTheyMentioned[friend2] # Insert into the friend's similarity vector\n",
    "                vectorDict[friend] = [yourVector, friendVector] # If the user is not protected, and friend's similarity vector is built, add it into the dictionary\n",
    "    return vectorDict \n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Function calculate_mention_similarity_dict(vectorDictionary) \n",
    "# ============================================================\n",
    "# Returns a dictionary of the cosine similarity score for each user the authenticating user mentioned\n",
    "# (i.e. {john: 0.53, anna: 0.4, bill: 0.17})\n",
    "# Argument: vectorDictionary is the vectorDict returned by build_vectors_for_who_mentioned_you method.\n",
    "def calculate_mention_similarity_dict(vectorDictionary):\n",
    "    similarityDict = {}\n",
    "    for friend in vectorDictionary:\n",
    "        bothVectors = vectorDictionary[friend]\n",
    "        yourVector = bothVectors[0]\n",
    "        theirVector = bothVectors[1]\n",
    "        result = 1-spatial.distance.cosine(yourVector, theirVector)\n",
    "        similarityDict[friend] = result\n",
    "    return similarityDict\n",
    "\n",
    "############################################\n",
    "# GET RETWEET DATA\n",
    "############################################\n",
    "# Note: This is still a prototype.\n",
    "# \n",
    "# The goal was to find every person who retweeted a tweet of the authenticating user.\n",
    "# i.e. [user1, user2, user3] = [1, 5, 6] (you can read this as: user1 retweeted 1, user2 retweeted 5 times, user3 retweeted 6 times)\n",
    "# Then, grab the retweeted tweets for every person who retweeted you, find who retweeted those retweets, and compare lists of retweeters\n",
    "# i.e. for user1, within all their retweets, here are the people who retweeted (and the number of times they retweeted): \n",
    "# [user4, user2, user7] = [1, 1, 1]\n",
    "# The most similar person/doppelganger will be the user who has the most retweeters in common with the authenticating user.\n",
    "# This will be calculated using cosine similarity.\n",
    "#\n",
    "#\n",
    "# Each function below is called in the final function: find_retweet_similarity\n",
    "#\n",
    "\n",
    "# myPopTweets(numTweets)\n",
    "# numTweets must <= 100\n",
    "# Returns a list of the ids of your popular tweets\n",
    "def myPopTweets(numTweets):\n",
    "    tweets = t.statuses.retweets_of_me(count=numTweets) #grab numTweets (some # of) tweets that others retweeted\n",
    "    popularTweets=[]\n",
    "    print(len(tweets))\n",
    "    for tweet in tweets:\n",
    "        #print(tweet['user']['screen_name']+\"\\n\")\n",
    "        #print(tweet['text']+\"\\n\")\n",
    "        #print(\"------ ----- ----- ----- -----\")\n",
    "        popularTweets.append(tweet['id'])\n",
    "    return popularTweets\n",
    "\n",
    "# Checks to see if the tweetID still exists.\n",
    "def isAccessible(tweetID):\n",
    "    try:\n",
    "        #results = t.statuses.retweeters.ids(screen_name=username) #API query#Wow you fucked up\n",
    "        results = t.statuses.retweeters.ids(_id=tweetID, stringify_ids=False)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "# whoRetweeted(tweetID)\n",
    "# Given a tweet id, returns a list of user ids that retweeted the tweet\n",
    "# Notice: Making a request to statuses/retweets/ids is a little tricky. \n",
    "# Work around: https://github.com/sixohsix/twitter/issues/300\n",
    "def whoRetweeted(tweetID):\n",
    "    users=[]\n",
    "    print(\"tweet ID: \"+str(tweetID)+\", type: \"+str(type(tweetID))+\"\\n\")\n",
    "    if isAccessible(tweetID) != False:\n",
    "        userList = t.statuses.retweeters.ids(_id=tweetID, stringify_ids=False)\n",
    "        print(\"size of user list: \"+str(len(userList['ids']))+\"\\n\")\n",
    "        for userID in userList['ids']:\n",
    "            print(\"--> userID from userList['ids']: \"+str(userID)+\"\\n\")\n",
    "            users.append(userID)\n",
    "    else:\n",
    "        print(\"Sorry! Retweeters of \"+str(tweetID)+\" is not accessible?\\n\")\n",
    "    return users\n",
    "\n",
    "# buildRetweetDictionary(tweetsList)\n",
    "# tweetsList will be a list of tweet IDs the authenticating user authored\n",
    "# Returns a dicitonary of key:value pair tweetID and userIDs.\n",
    "def buildRetweetDictionary(tweetsList):\n",
    "    tweetUID = {} #{TID: [UID, UID, ... , UID], TID: [UID, UID ... , UID]} etc\n",
    "    userIDList = []\n",
    "    for tweetID in tweetsList:\n",
    "        userIDList = whoRetweeted(tweetID) # Get a list of the userIDs who retweeted the particular tweet ID\n",
    "        tweetUID[tweetID] = userIDList # Throw it into a dictionary\n",
    "    return tweetUID\n",
    "\n",
    "# Returns frequency list, rather than a dictionary, of user ids that retweeted your tweets.\n",
    "def build_simple_list(tweetUID): #merge all lists (we are merging a dictionary of lists)\n",
    "    mergedList = []\n",
    "    for tweetID in tweetUID:\n",
    "        usersList = tweetUID[tweetID]\n",
    "        mergedList=mergedList+usersList\n",
    "    return mergedList\n",
    "\n",
    "# Checks to see if a given user is active based on the user id.\n",
    "def isActive(uid):\n",
    "    try:\n",
    "        lookup = t.users.lookup(user_id=uid)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "        \n",
    "# convert_UID_to_names\n",
    "# Given a merged list, for example ...\n",
    "# mergedList = [UID1, UID1, UID2, UID3, UID4, UID4, UID4]\n",
    "# Create a dictionary of UID and # of times it appears in the merged list\n",
    "def convert_UID_to_names(mergedList):\n",
    "    convertedDict = {}\n",
    "    for uid in mergedList: #this is a very slow way of getting the job done (hoepfully it works?)\n",
    "        check = isActive(uid) #check if the user is active\n",
    "        if check != True:\n",
    "            print(\"This user with the id \"+uid+\" is not active. We can't look them up.\\n\")\n",
    "        else:\n",
    "            lookupUser = t.users.lookup(user_id=uid)\n",
    "            for userDict in lookupUser:\n",
    "                username = userDict['screen_name']\n",
    "                if username in convertedDict:\n",
    "                    convertedDict[username] += 1\n",
    "                else:\n",
    "                    convertedDict[username] = 1\n",
    "    return convertedDict\n",
    "\n",
    "# find_retweet_similarity\n",
    "# Calls all the data gathering methods to calculate your retweet similarity\n",
    "def find_retweet_similarity():\n",
    "    popularTweets = myPopTweets(100) # Grab your popular tweets (returns a list of tweet ids)    \n",
    "    tweetUID=buildRetweetDictionary(popularTweets) # Builds a dictionary of tweet ids, and list of users who retweeted it\n",
    "    #{TID: [UID, UID, ... , UID], TID: [UID, UID ... , UID]} etc\n",
    "    mergedListUID = build_simple_list(tweetUID) # Builds a list of user ids that retweeted your tweet: i.e. [123124, 488177, 283177, 2398100, etc...]\n",
    "    convertedDict = convert_UID_to_names(mergedListUID)\n",
    "    retweetDict = {}\n",
    "    for name in convertedDict:\n",
    "        theirRTs = get_retweets_of(name) # Get the popular tweets of the name user\n",
    "        theirTweetUID = buildRetweetDictionary(theirRTs)\n",
    "        theirMergedList = buildSimpleList(theirTweetUID)\n",
    "        theirConvertedDict = convertUIDtoNames(theirMergedList)\n",
    "        retweetDict[name] = theirConvertedDict #i.e. {username1: 4, username2:6}\n",
    "    buildVectorsRT(convertedDict, retweetDict)\n",
    "    \n",
    "#Makes a list of zeros ready for vector building.\n",
    "def zero_list_maker(n):\n",
    "    listofzeros = [0] * n\n",
    "    return listofzeros           \n",
    "    \n",
    "# Finds the retweeted tweets of a given user.\n",
    "# Returns a list of the tweet IDs. \n",
    "def get_retweets_of(username):\n",
    "    theirRTs = []\n",
    "    theirTweets = t.statuses.user_timeline(screen_name=username, count=200, include_rts=True)\n",
    "    for status in theirTweets:\n",
    "        RT =status['retweet_count'] #Notice: 'RT = status[\"retweeted\"]' does not appear to grab all retweeted tweets\n",
    "        if RT > 0:\n",
    "            TID = status['id'] # Grab the ID of the tweet\n",
    "            theirRTs.append(TID)\n",
    "    return theirRTs\n",
    "\n",
    "# buildVectorsRT(yourDictionary, theirDictionaries)\n",
    "# yourDictionary = {username: #, username: #, username: # ... etc}, where each value is the number of times this particular user\n",
    "# retweeted your tweets.\n",
    "# theirDictionaries is a dictionary of dictionaries. Each user inside the inner dictionary value has retweeted tweets authored by the key user.\n",
    "# {username1: {username1: #, username2: #...}, username2: {etc}, username3: {etc}}\n",
    "def buildVectorsRT(yourDictionary, theirDictionaries, username):\n",
    "    \n",
    "    finalDict = {} #A dictionary to return that holds all the final vectors for each user and the authenticating user\n",
    "    \n",
    "    # Sort your dictionary to keep its order\n",
    "    yourDictionary = collections.OrderedDict(sorted(yourDictionary.items(), key=lambda t: t[1]))\n",
    "    \n",
    "    # sort all the dictionaries inside the dicitonary\n",
    "    for usr in theirDictionaries:\n",
    "        dic = theirDictionaries[usr]\n",
    "        dic = collections.OrderedDict(sorted(dic.items(), key=lambda t: t[1]))\n",
    "        theirDictionaries[usr] = dic\n",
    "    \n",
    "    #order all the dicitonaries in the dicitonary\n",
    "    theirDictionaries = collections.OrderedDict(sorted(theirDictionaries.items(), key=lambda t: t[1]))\n",
    "    \n",
    "    yourVector = []\n",
    "    yourVector = zero_list_maker(len(yourDictionary))\n",
    "    # Build your vector\n",
    "    for f in yourDictionary:\n",
    "        friendIndex = yourDictionary.keys().index(f) #Grab their index\n",
    "        yourVector[friendIndex] = yourDictionary[f]\n",
    "    finalDict[username]=yourVector # Add it to the final dictionary\n",
    "    \n",
    "    # Build vectors for each user who retweeted your tweets\n",
    "    for u in theirDictionaries:\n",
    "        theirVector = []\n",
    "        theirVector = zerolistmaker(len(yourDictionary))\n",
    "        d = theirDictionaries[u]#grab the dictionary\n",
    "        for u2 in d:\n",
    "               if u2 in yourDictionary:\n",
    "                    uIndex = yourDictionary.keys().index(u2) #grab their index in your dicitonary\n",
    "                    uScore = d[u2] #grab the score\n",
    "                    theirVector[uIndex] = uScore\n",
    "        finalDict[u] = theirVector\n",
    "    return finalDict\n",
    "\n",
    "# find_retweet_similarity\n",
    "# returns a dictionary of username and similarity vectors for cosine similarity calculation to run on\n",
    "def find_retweet_similarity(username):\n",
    "    popularTweets = myPopTweets(100) #grab your popular tweets (returns a list of tweet ids)    \n",
    "    tweetUID=buildRetweetDictionary(popularTweets) #builds a dictionary of tweet ids, and list of users who retweeted it\n",
    "    mergedListUID = buildSimpleList(tweetUID)\n",
    "    convertedDict = convertUIDtoNames(mergedListUID) #dont convert usernames without an error handling where users are non existant\n",
    "    retweetDict = {}\n",
    "    for name in convertedDict:\n",
    "        theirRTs = getRetweetsOf(name)#get the popular tweets of the name user\n",
    "        theirTweetUID = buildRetweetDictionary(theirRTs)\n",
    "        theirMergedList = buildSimpleList(theirTweetUID)\n",
    "        theirConvertedDict = convertUIDtoNames(theirMergedList)\n",
    "        retweetDict[name] = theirConvertedDict #{ann: 4, john:6}\n",
    "    finalDict = buildVectorsRT(convertedDict, retweetDict, username)\n",
    "    return finalDict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# 1.3 CALCULATION\n",
    "# The synthesis of all three measurements of similarity.\n",
    "########################################################\n",
    "\n",
    "############################################\n",
    "# BUILD COSINE SIMILARITIES\n",
    "############################################\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stop_word_list = get_stop_words('en')\n",
    "\n",
    "def get_profile_corpus(my_user_name, info_type):\n",
    "    corpus = []\n",
    "    if (info_type == \"description\"):\n",
    "        info_dict = description_dict\n",
    "    else:\n",
    "        info_dict = location_dict\n",
    "    corpus.append(info_dict[my_user_name])\n",
    "    for user in info_dict:\n",
    "        if user != my_user_name:\n",
    "            corpus.append(info_dict[user])\n",
    "    return corpus\n",
    "\n",
    "def get_tweet_corpus(my_user_name, just_hash_tags):\n",
    "    corpus = []\n",
    "    my_content = \"\"\n",
    "    \n",
    "    # need to have current user content as the first in the document array/corpus\n",
    "    # this would make it easier to calculate cosine similarity between\n",
    "    # tf-idf vectors later\n",
    "    for tweet in tweet_dict[my_user_name]:\n",
    "        if (just_hash_tags):\n",
    "            hashTags = tweet['entities']['hashtags']\n",
    "            if (len(hashTags) != 0):\n",
    "                for hashTag in hashTags:\n",
    "                    my_content += (hashTag['text'] + \" \")\n",
    "        else:\n",
    "            my_content += (tweet['text'] + \" \")\n",
    "\n",
    "    corpus.append(my_content)\n",
    "    \n",
    "    # now add friends' content to the corpus\n",
    "    for user in tweet_dict:\n",
    "        if user != my_user_name:\n",
    "            current_user_content = \"\"\n",
    "            for tweet in tweet_dict[user]:\n",
    "                if (just_hash_tags):\n",
    "                    hashTags = tweet['entities']['hashtags']\n",
    "                    if (len(hashTags) != 0):\n",
    "                        for hashTag in hashTags:\n",
    "                            current_user_content += (hashTag['text'] + \" \")\n",
    "                else:\n",
    "                    current_user_content += (tweet['text'] + \" \")\n",
    "            corpus.append(current_user_content)\n",
    "\n",
    "    return corpus\n",
    "\n",
    "def get_cosine_similarities(corpus, my_user_name):\n",
    "    # transform the documents into tf-idf vectors, then compute the cosine similarity between them\n",
    "    # method taken from here: http://stackoverflow.com/questions/8897593/similarity-between-two-text-documents\n",
    "    tfidf = TfidfVectorizer(stop_words = stop_word_list).fit_transform(corpus)\n",
    "    pairwise_similarity = tfidf * tfidf.T\n",
    "    cosine_similarities_list = pairwise_similarity.A[0]\n",
    "    cosine_similarities_dict = defaultdict(list)\n",
    "    index = 1\n",
    "    for user in tweet_dict:\n",
    "        if user != my_user_name:\n",
    "            cosine_similarities_dict[user] = cosine_similarities_list[index]\n",
    "            index += 1\n",
    "    return cosine_similarities_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "# 1.4 FINAL RETURN\n",
    "# Calculates final similarity\n",
    "#############################\n",
    "\n",
    "def get_all_cosine_similarities(user_name, contentWeight, profileWeight, networkWeight):\n",
    "    all_cosine_similarities_dict = defaultdict(float)\n",
    "    \n",
    "    tweet_corpus = get_tweet_corpus(user_name, False)\n",
    "    content_cosine_similarities_dict = get_cosine_similarities(tweet_corpus, user_name)\n",
    "\n",
    "    description_corpus = get_profile_corpus(user_name, \"description\")\n",
    "    description_cosine_similarities_dict = get_cosine_similarities(description_corpus, user_name)\n",
    "\n",
    "    location_corpus = get_profile_corpus(user_name, \"location\")\n",
    "    location_cosine_similarities_dict = get_cosine_similarities(location_corpus, user_name)\n",
    "    \n",
    "    yourMentions = find_who_you_mentioned_in_your_tweets(user_name)\n",
    "    whoMentionedYou = find_who_mentioned_you(200) #TO DO\n",
    "    fDict = buildVectorsForWhoMentionedYouCorrectly(yourMentions)\n",
    "    fDict = cleanVectorDictionary(fDict)\n",
    "    md = mentionSimilarityBetter(fDict)\n",
    "\n",
    "    for user in content_cosine_similarities_dict:\n",
    "        content_score = float(content_cosine_similarities_dict[user])\n",
    "        description_score = float(description_cosine_similarities_dict[user])\n",
    "        location_score = float(location_cosine_similarities_dict[user])\n",
    "        # have to check because number users in mention list is much smaller\n",
    "        if user in md:\n",
    "            mention_score = float(md[user])\n",
    "        else:\n",
    "            mention_score = 0\n",
    "        profile_score = description_score * 0.5 + location_score * 0.5\n",
    "        # add retweet score here if necessary\n",
    "        network_score = mention_score\n",
    "        \n",
    "        all_score = content_score * contentWeight + profile_score * profileWeight + network_score * networkWeight\n",
    "        all_cosine_similarities_dict[user] = all_score\n",
    "    return all_cosine_similarities_dict\n",
    "\n",
    "def findDoppelganger(username, x, y, z):\n",
    "    all_cosine = get_all_cosine_similarities(username, x, y, z)\n",
    "    all_cosine_sorted = sorted(all_cosine.iteritems(),key=lambda (k,v): v,reverse=True)\n",
    "    print all_cosine_sorted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "findDoppelganger('Macalester', 2.0, 4.0, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# 2. QUANTITATIVE ANALYSIS\n",
    "########################################################\n",
    "\n",
    "# ###################################################\n",
    "# # BUILD MENTIONS - COSINE SIMILARITIES SCATTER PLOT\n",
    "# ###################################################\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['backend'] = \"Qt4Agg\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ======================================================\n",
    "# Function find_who_you_mentioned_in_your_tweets(userA)\n",
    "# ======================================================\n",
    "# find_who_you_mentioned_in_your_tweets(userA)\n",
    "# Returns a dictionary of users that userA mentioned and number of times they were mentioned\n",
    "# Reads in a username for userA (i.e. \"gozonglor\", string)\n",
    "# i.e. {@bob: 1, @ann: 4, @cary: 2, etc} --> userA mentioned bob 1 time, ann 4 times, and cary 2 times.\n",
    "def find_who_you_mentioned_in_your_tweets(userA):\n",
    "    # SET UP\n",
    "    # Notice below: 'count' is hard-coded to be 200. 200 is the Twitter API limit. More tweets, more mentions.\n",
    "    yourTweets = t.statuses.user_timeline(screen_name=userA, count=200, include_rts=\"false\") \n",
    "    yourMentions = {} #Initialize a dictionary to store all your mentioned users\n",
    "    \n",
    "    # BUILD DICTIONARY OF MENTIONS\n",
    "    for status in yourTweets: # For each of your tweets\n",
    "        if \"@\" in status['text']: # If there's an @ symbol (a mention)\n",
    "            cleanTweet = status['text'].strip() # Clean the entire tweet of any unnecessary white space\n",
    "            tokens = cleanTweet.split(\" \") # Split the tweet into words\n",
    "            for word in tokens: # For each word\n",
    "                if len(word) > 1: # If it's longer than 1 character\n",
    "                    wordS = word.split() # Split the word into individual characters (i.e. from \"@bob\" to [\"@\",\"b\",\"o\",\"b\"])\n",
    "                    if word[0][:1] == \"@\": # If the first character is an @\n",
    "                        if word == \"@@\": # Error handling for typo in a mention?\n",
    "                            break\n",
    "                        if word in yourMentions:\n",
    "                            yourMentions[word] += 1\n",
    "                        else:\n",
    "                            yourMentions[word] = 1\n",
    "                            \n",
    "    # IF yourMentions REMAINS EMPTY...\n",
    "    if len(yourMentions) == 0: \n",
    "        print(\"Sorry, \"+userA+\" has not mentioned anyone yet.\")\n",
    "    return yourMentions\n",
    "\n",
    "# ============================================\n",
    "# Function find_who_mentioned_you(numTweets)\n",
    "# ============================================\n",
    "# Returns a dictionary of users and number of times they mentioned the authenticating user.\n",
    "# numTweets are the number of tweets to pull that mention the AUTHENTICATING user\n",
    "# Twitter API restricts numTweets to be >= 200. \n",
    "def findWhoMentionedYou(numTweets):\n",
    "    # SET UP\n",
    "    tweets = t.statuses.mentions_timeline(count=numTweets)\n",
    "    mentionedBy = {}\n",
    "    \n",
    "    # FIND USERS WHO MENTIONED AUTHENTICATING USER\n",
    "    if len(tweets) == 0:\n",
    "        print(\"Sorry, no mentions of the authenticated user.\")\n",
    "    else:\n",
    "        for tweetObject in tweets: \n",
    "            tweeter = tweetObject['user']['screen_name'] # Tweeter is the user who mentioned your authenticating user\n",
    "            if tweeter in mentionedBy:\n",
    "                mentionedBy[tweeter] += 1\n",
    "            else:\n",
    "                mentionedBy[tweeter] = 1\n",
    "    return mentionedBy\n",
    "\n",
    "# ========================================================================\n",
    "# Function linear_regression_counts(whoYouMentioned, usersWhoMentionYou)\n",
    "# ========================================================================\n",
    "#returns a dictionary of total mentions between you ('gozonglor') and a user\n",
    "#whoYouMentioned should be a dictionary returned by the function find_who_you_mentioned_in_your_tweets('gozonglor') *defaults/hard coded to request 200 tweets\n",
    "#usersWhoMentionYou should be a dictionary returned by the function find_who_mentioned_you(count) *count = 200, the max number of returns from mentions_timeline\n",
    "def linear_regression_counts(whoYouMentioned, usersWhoMentionYou):\n",
    "    totalDict = {} \n",
    "    for user in whoYouMentioned:\n",
    "        numMentions = whoYouMentioned[user] #grab the user you mentioned\n",
    "        user = user.split(\"@\")\n",
    "        user = user[1]\n",
    "        if user in usersWhoMentionYou: #check if they also mentioned you\n",
    "            theirMention = usersWhoMentionYou[user]\n",
    "            totalDict[user] = numMentions+theirMention\n",
    "    return totalDict\n",
    "\n",
    "yourMentions = find_who_you_mentioned_in_your_tweets(\"\")\n",
    "whoMentionedYou = find_who_mentioned_you(200)\n",
    "linearDict = linear_regression_counts(yourMentions, whoMentionedYou)\n",
    "    \n",
    "mentionsVector = []\n",
    "cosineScoreVector = []\n",
    "for user in lDict:\n",
    "    mentionsVector.append(linearDict[user])\n",
    "    cosineScoreVector.append(content_cosine_similarities_dict[user])\n",
    "\n",
    "plt.scatter(mentionsVector, cosineScoreVector,  color='blue')\n",
    "\n",
    "plt.plot(mentionsVector, np.poly1d(np.polyfit(mentionsVector, cosineScoreVector, 1))(mentionsVector))\n",
    "\n",
    "plt.xticks(np.arange(min(mentionsVector), max(mentionsVector), 2))\n",
    "plt.yticks(np.arange(0,1, 0.1))\n",
    "plt.xlabel(\"Mentions\")\n",
    "plt.ylabel(\"Cosine similarity\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
